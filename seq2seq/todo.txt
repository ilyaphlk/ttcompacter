####################
Замер экспрессивности модели
####################

Нужно как-то мерить экспрессивность?
то, что экспрессивность повышается с добавлением нелинейности, обосновано в статье On the Expressive Power of Deep Neural Networks




#####################
LoRA-style подключение
#####################

Видимо, менять в самом t5_modeling.py
Нужно сделать так, чтобы на стадии инференса эти веса перемножались в обычную матрицу и складывались с параллельной

нужно раскладывать именно Linear, то есть добавлять слои в T5DenseReluDense (на GeLU-блок пока забиваем, если что - по аналогии)

if self.training:
  параллельный запуск
else:
  параллельная ветка не вычисляется в предположении, что
  веса уже распакованы и прибавлены к основным

тогда добавить 2 функции:
1) вызывается следом за model.eval(): распаковывает веса и прибавляет их к оригинальным
2) вызывается следом за model.train(): вычитает из оригинальных добавленные

ещё нужно в modify_model_after_init сделать заморозку линейных внутри FF, но разморозку TTLinear

FF -> forward

modeling_t5:314:
- поставить and not use_lora_style при запуске адаптера
? сделать forward через параллельный запуск

и везде сделать прокидку параметров в конфиги
(configuration_t5, например)



#####################
LayerNorm всех сортов и расцветок
#####################

1) просто отключить обучение LayerNorm - baseline
2) сделать ScaleNorm
3) заменить LayerNorm TT-слоем и обучить с нуля
4) сделать разложение существующего LayerNorm
5) ? LayerNorm из предложенного в compacter?


ScaleNorm:
- добавить арги в конфиг

TTLayerNorm:
- добавить арги в конфиг
- выписать всё в отдельный класс, чтобы не ифать
t3nsor/layers.py - сделать TTLayerNorm
initializers.py - random_matrix, matrix_with_random_cores, const_initializer
__init__.py - import const_initializer

import в modeling_t5.py
- понять, какая должна быть инициализация, чтобы в начале вектор был единичным
(ones(...) / tt_rank ** ((d-1)/d))







modeling_t5:233 - 


#######################
Если не будет хватать
#######################
Можно зафигачить Low Resource setting, как они в статье делают
Можно по бырому прогнать всё это для SuperGLUE (?)



#####################
Tensor Basis + TR
#####################

Вообще фиг знает









####################
Текст Диплома:
####################
В Method подробнее описать Tensor Train, например, описать концепцию тензорного ранга (?)

TTLayerNorm - недостающее звено между ScaleNorm и LayerNorm по числу обучаемых параметров





adapters/adapter_modeling.py:
- Добавить свой TensorTrainAdapter


adapters/tensor_train_linear.py:
- Добавить свой TensorTrainLinear:
   - конфиг должен уметь решейпиться под заданное число ядер - 30 минут
   - как-то указывать, какие ядра фризятся, а какие нет?


adapters/adapter_controller.py:
. Добавить TensorTrainAdapter в места, где проверяется config.tensor_train_adapters==True


adapters/adapter_configuration.py:
. Добавить поле tensor_train_adapters = False
. Добавить поля для конфигурации TensorTrainAdapter
   - [инициализация? число ядер? переиспользование ядер?]

- third_party/../modeling_t5.py:
? Добавить обработку TTAdapter?

- utils.py:
? обработка слоёв внутри freeze_model_params
? добавление множественного tasks в get_adapter_config

- task_expansion_factor - добавить везде, где есть обработка task_reduction_factor


**********
Логирование:
**********
- run_seq2seq.py
  - Добавить лог метрик в wandb - везде где есть logger.
  	  - 
      [Можно сделать с помощью run = wandb.init() if config.log_to_wandb else FakeWandb()]
      [FakeWandb() def log(d): pass]
  - Везде где есть trainer.log
  ? Добавить измерение памяти по слоям
  - run.config - не проходить по всем атрибутам, выделить основные
  - config.experiment_name = 
  - config.output_dir = f"outputs/{config.experiment_name}"


************
Выключение ядер:
************

t3nsor/layers.py:
- Добавить аргумент, выключающий отдельные ядра

initializers.py/matrix_with_random_cores:
- Добавить проброску аргумента в конструктор TensorTrain

t5_modeling.py (Или какой-то ещё файл):
- Подсмотреть, как шерятся веса в компактере



seq2seq.data:
- Реализация пайплайна, в котором модель учится на данных вперемешку




************
Для себя:
************
Запустить TTLinear с разными аргументами, посмотреть, как он делает shape, что у него внутри


************
Идеи экспериментов:
************
- вставить промежуточные активации в перемножение ядер? Мотивация: поскольку раскладываем слой на несколько малых, есть возможность искуственно сделать модель глубже, что по наблюдениям (ссылка) делает модель экспрессивнее
